% Encoding: UTF-8
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@misc{Python,
        title = {Python Software Foundation, Python Language Reference, version
                3.7. Available at},
        note = {\url{http://www.python.org/}}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@misc{PyRO,
        title = {General Python FAQ},
        note = {\url{https://docs.python.org/3/faq/general.html#what-is-python},
                accessed {19/05/19}}
}

@article{DBLP:journals/corr/abs-1804-03209,
  author    = {Pete Warden},
  title     = {Speech Commands: {A} Dataset for Limited-Vocabulary Speech Recognition},
  journal   = {CoRR},
  volume    = {abs/1804.03209},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.03209},
  archivePrefix = {arXiv},
  eprint    = {1804.03209},
  timestamp = {Mon, 13 Aug 2018 16:48:32 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-03209},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/AmodeiABCCCCCCD15,
  author    = {Dario Amodei and
               Rishita Anubhai and
               Eric Battenberg and
               Carl Case and
               Jared Casper and
               Bryan Catanzaro and
               Jingdong Chen and
               Mike Chrzanowski and
               Adam Coates and
               Greg Diamos and
               Erich Elsen and
               Jesse H. Engel and
               Linxi Fan and
               Christopher Fougner and
               Tony Han and
               Awni Y. Hannun and
               Billy Jun and
               Patrick LeGresley and
               Libby Lin and
               Sharan Narang and
               Andrew Y. Ng and
               Sherjil Ozair and
               Ryan Prenger and
               Jonathan Raiman and
               Sanjeev Satheesh and
               David Seetapun and
               Shubho Sengupta and
               Yi Wang and
               Zhiqian Wang and
               Chong Wang and
               Bo Xiao and
               Dani Yogatama and
               Jun Zhan and
               Zhenyao Zhu},
  title     = {Deep Speech 2: End-to-End Speech Recognition in English and Mandarin},
  journal   = {CoRR},
  volume    = {abs/1512.02595},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.02595},
  archivePrefix = {arXiv},
  eprint    = {1512.02595},
  timestamp = {Mon, 22 Jul 2019 13:51:23 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/AmodeiABCCCCCCD15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Williamsong,
  author    = {William Song
               Jim Cai},
  title     = {End-to-End Deep Neural Network for AutomaticSpeech Recognition},
  year      = {2015},
  url       = {https://cs224d.stanford.edu/reports/SongWilliam.pdf},
}

@misc{mfcc,
        title = {Mel Frequency Cepstral Coefficient (MFCC) tutorial},
        note =
        {\url{http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/#references},
        accessed {02/12/19}}
}
@article{doi:10.1162/neco.1997.9.8.1735,
  author = {Hochreiter, Sepp and Schmidhuber, J√ºrgen},
  title = {Long Short-Term Memory},
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735-1780},
  year = {1997},
  doi = {10.1162/neco.1997.9.8.1735},
  URL = {https://doi.org/10.1162/neco.1997.9.8.1735},
  eprint = {https://doi.org/10.1162/neco.1997.9.8.1735},
  abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. }
}
@article{DBLP:journals/corr/ChungGCB14,
  author    = {Junyoung Chung and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               KyungHyun Cho and
               Yoshua Bengio},
  title     = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1412.3555},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.3555},
  archivePrefix = {arXiv},
  eprint    = {1412.3555},
  timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChungGCB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{librosa,
  autor     = {Brian McFee, Vincent Lostanlen, Matt McVicar, Alexandros Metsai,
               Stefan Balke, Carl Thomé, Adam Weiss},
  version   = {0.7.1},
  url       = {http://doi.org/10.5281/zenodo.3478579}
}
@misc{Adam,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
a variant of Adam based on the infinity norm.},
  added-at = {2019-06-04T16:24:16.000+0200},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/2d53bcfff0fe1a1d3a4a171352ee6e92c/alrigazzi},
  description = {Adam: A Method for Stochastic Optimization},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {d53bcfff0fe1a1d3a4a171352ee6e92c},
  keywords = {deep dl large-scale networks neural},
  note = {cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference  for Learning Representations, San Diego, 2015},
  timestamp = {2019-06-04T16:24:16.000+0200},
  title = {Adam: A Method for Stochastic Optimization},
  url = {http://arxiv.org/abs/1412.6980},
  year = 2014
}
@misc{teachablemachine,
  title = {Teachable Machine},
  note = {\url{https://teachablemachine.withgoogle.com/}, 
          accessed: {23/12/19}}
}
Comment{jabref-meta: databaseType:bibtex;}
