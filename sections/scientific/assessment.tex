% corrected LN 87

\subsection{Assessment}
\subsubsection{NFR01: Performance evaluation}~\label{assessment}~\\

In this section, the performance of the four ANNs investigated in
Section~\ref{ann} are evaluated. We use the English and Luxembourgish dataset
for the performance analysis.\\

The English and Luxembourgish dataset contains spoken words $w \in
\{'0',\dots,'9'\}$. The English dataset consists of 17749 data instances whereas
the Luxembourgish consists of 500 data instances.\\

The first step is to choose an optimal optimizer. The choice is between using
the stochastic gradient descent (SGD) or Adam optimizer. The latter is a
combination of two extensions of SGD, Adaptive Gradient Algorithm (AdaGrad) and
Root Mean Square Propagation (RMSProp)~\cite{Adam}. The four models will be
trained on the English dataset to designate the right optimizer and number of
epochs. Their training will be visualized with $60\ epochs$ each. The training
visualization is given in the appendix as follows:\\

\begin{enumerate}[label=\arabic*.]
  \item Feedforward Neural Network:
    \begin{itemize}
      \item Validation accuracy in Figure~\ref{ffa}.
      \item Validation loss in Figure~\ref{ffl}.
    \end{itemize}
  \item RNN:
    \begin{itemize}
      \item Validation accuracy in Figure~\ref{rnna}.
      \item Validation loss in Figure~\ref{rnnl}.
    \end{itemize}
  \item LSTM:
    \begin{itemize}
      \item Validation accuracy in Figure~\ref{lstma}.
      \item Validation loss in Figure~\ref{lstml}.
    \end{itemize}
  \item GRU:
    \begin{itemize}
      \item Validation accuracy in Figure~\ref{grua}.
      \item Validation loss in Figure~\ref{grul}.
    \end{itemize}
\end{enumerate}~\\

Let $n=60$ be the number of epochs. The two optimizers return the following
testing accuracies (acc) and losses:

\begin{table}[H]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        \multirow{2}{*}{Optimizer} & \multicolumn{2}{c|}{FF} &
        \multicolumn{2}{c|}{RNN} & \multicolumn{2}{c|}{LSTM} &
        \multicolumn{2}{c|}{GRU} \\
        \cline{2-9}
        & {acc} & {loss} & {acc} & {loss} & {acc} & {loss} & {acc} & {loss}\\
        \hline
        SGD & 0.79 & 0.65 & 0.43 & 1.51 & 0.86 & 0.46 & 0.85 & 0.46 \\ \hline
        Adam & 0.87 & 0.47 & 0.72 & 0.97 & 0.94 & 0.24 & 0.94 & 0.24 \\ \hline
    \end{tabular}%
    \caption{Comparison of the four models' testing results using SGD and Adam.
    The models were trained on the English dataset.}
    \label{table:comparison}
    }
\end{table}

Table~\ref{table:comparison} shows that the validation accuracies using the Adam
optimizer yield better accuracies compared to SGD. Therefore, for the following
investigation, the Adam optimizer is considered.\\

The next step is to choose the number of epochs which will be used for the
performance analysis. The training visualizations show the loss converges around
$40$ epochs except for the RNN model. Figure \ref{300rnna} shows that the RNN
model converges around $220$ epochs with an accuracy of 0.88. The training
visualization of the RNN model with $300$ epochs is given in
Figures~\ref{300rnna} and~\ref{300rnnl} in the appendix.\\

Choosing $epochs = 40$ to evaluate each model's performance, we have:

\begin{table}[H]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        \multirow{2}{*}{Optimizer} & \multicolumn{2}{c|}{FF} &
        \multicolumn{2}{c|}{RNN} & \multicolumn{2}{c|}{LSTM} &
        \multicolumn{2}{c|}{GRU} \\
        \cline{2-9}
                                 & {acc} & {loss} & {acc} & {loss} & {acc} &
        {loss} & {acc} & {loss}\\ \hline
        Adam & 0.83 & 0.54 & 0.63 & 1.10 & 0.937 & 0.23 & 0.94 &
        0.23 \\ \hline
    \end{tabular}%
    \caption{The testing results of the four models using Adam. The four models
    were trained on the English dataset. $FF$ is the abbreviation for feedforward
    and $acc$ stands for accuracy.}
    \label{table:40}
    }
\end{table}

Inspecting the results shows that LSTM and GRU perform better than feedforward
and RNN. The overall performance is slightly underneath the performance at 60
epochs. In this case, RNN has the lowest accuracy. This is expected, since
standard RNNs can't deal with learning long-term temporal dependencies, because
the gradient of the loss function decays exponentially with time (vanishing
gradient problem). LSTMs and GRUs use gated units to maintain the information in
memory for longer periods of time, and to control which information is kept, and
which is forgotten. This explains substantially better performance obtained
using LSTM and GRU. Compared to feedforward neural networks, LSTM and GRU have
better accuracy since they take advantage of their recurrent units to process
series data.\\

\textbf{Training on Luxembourgish dataset.} The same models used to train on the
English dataset were used to train on the Luxembourgish dataset. The following
testing accuracies and losses were obtained while training on the Luxembourgish
dataset:

\begin{table}[H]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        \multirow{2}{*}{Optimizer} & \multicolumn{2}{c|}{FF} &
        \multicolumn{2}{c|}{RNN} & \multicolumn{2}{c|}{LSTM} &
        \multicolumn{2}{c|}{GRU} \\
        \cline{2-9}
        & {acc} & {loss} & {acc} & {loss} & {acc} & {loss} & {acc} & {loss}\\
        \hline
        Adam & 0.84 & 0.59 & 0.73 & 0.83 & 1.0 & 0.02 & 0.99 & 0.02 \\ \hline
    \end{tabular}%
    \caption{The testing results of the four models using Adam. The four models
    were trained on the Luxembourgish dataset. The dataset contains 50 instances
    of each spoken word. FF is the abbreviation for feedforward and $acc$ stands for
    accuracy.}
    \label{table:lux}
    }
\end{table}

The results show that LSTM and GRU generalize better than feedforward and RNN.
The validation accuracies of LSTM and GRU are both roughly 99\%. The high
accuracy of LSTM and GRU can be explained by the small dataset which is also
speaker-dependent.  The results of LSTM and GRU are expected to drop if the
dataset grows and contains a variety of speakers.\\

The training visualization on the Luxembourgish dataset is given in the appendix
as follows:\\

\begin{enumerate}[label=\arabic*.]
  \item Feedforward Neural Network:
    \begin{itemize}
      \item Validation accuracy in Figure~\ref{luxffa}.
      \item Validation loss in Figure~\ref{luxffl}.
    \end{itemize}
  \item RNN:
    \begin{itemize}
      \item Validation accuracy in Figure~\ref{luxrnna}.
      \item Validation loss in Figure~\ref{luxrnnl}.
    \end{itemize}
  \item LSTM:
    \begin{itemize}
      \item Validation accuracy in Figure~\ref{luxlstma}.
      \item Validation loss in Figure~\ref{luxlstml}.
    \end{itemize}
  \item GRU:
    \begin{itemize}
      \item Validation accuracy in Figure~\ref{luxgrua}.
      \item Validation loss in Figure~\ref{luxgrul}.
    \end{itemize}
\end{enumerate}
