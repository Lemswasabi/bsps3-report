% corrected VD 80

\subsubsection{Gated Recurrent Unit}~\\

Identical to LSTM units, the gated recurrent unit (GRU) consists of gated units
which control the flow of information through the unit without having a
dedicated memory cell. Compared to LSTM units, GRUs don't have an output gate.
The output gate is responsible to control the amount of memory content seen by
the other units in the network. Thus GRU exposes its entire content without
control.~\cite{DBLP:journals/corr/ChungGCB14}\\

The activation of a GRU unit is computed with:
\begin{equation}
  h_t = (1-z_t)h_{t-1} + z_t\tilde{h}_t
\end{equation}
with an update gate $z_t$ which decides how much the unit updates its content.
Its content consists of the previous activation $h_{t-1}$ and the candidate
activation $\tilde{h}_t$. The update gate is given by the equation:
\begin{equation}
  z_t=\sigma(W_z x_t+U_z h_{t-1}) 
\end{equation}
This update gate is similar to the one used by LSTM. However, it exposes its
state each time step since it does not have a procedure to control how the state
is exposed. The candidate activation is given by:
\begin{equation}
  \tilde{h}_t=\tanh(Wx_t+U(r_t\odot h_{t-1}))
\end{equation}
Where $r_t$ is a reset gate and is computed comparably to the update gate:
\begin{equation}
  r_t=\sigma(W_r x_t+U_r h_{t-1})
\end{equation}

See the illustration of a GRU uni in Figure~\ref{grucell}.

\input{sections/scientific/fr2/grucell.tex}
