% corrected LN 93

\subsubsection{Recurrent Neural Network}~\\

%introduction
Recurrent Neural Networks (RNNs) are a particular architecture of ANNs which can
process sequential data $S = x^{(1)},\dots,x^{(\tau)}$. To explain how RNNs are
implemented, we have to introduce the notion of computational graphs.
A computational graph is a formal structure representing a set of computations.
We obtain a chain of events by unfolding a recurrent computation into a
computational graph with a repetitive structure. \\

Consider a recurrent system,
\begin{equation}
  s^{(t)} = f(s^{(t-1)};\theta)
\end{equation}
with $s$ being the state of the system. The graph can be unfolded considering
the time step $\tau = 3$, we have,
\begin{equation}
  \begin{split}
    s^{(3)} & = f(s^{(2)};\theta) \\
          & = f(f(s^{(1)};\theta);\theta)
  \end{split}
  \label{recurrentsystem}
\end{equation}
This expression can be illustrated by a directed acyclic computational graph.
See figure~\ref{dag}\\
\input{sections/scientific/fr2/DAG.tex}

Now we consider recurrent system accepting an external signal $\bm{x}^{(t)}$,
\begin{equation}
  s^{(t)} = f(s^{(t-1)},\bm{x}^{(t)};\theta)
  \label{eqaccepting}
\end{equation}
where the state $s$ is containing information about the past sequence $S$. The
hidden units can be describe with equation~\ref{eqaccepting} using $\bm{h}$ to
denote the state,
\begin{equation}
  \bm{h}^{(t)} = f(\bm{h}^{(t-1)},\bm{x}^{(t)};\theta)
\end{equation}
represented in figure~\ref{unfoldrnnaccepting}.\\
\input{sections/scientific/fr2/unfoldrnnaccepting.tex}

%math

With graph unfolding, we can now present some common example of an RNNs. Here are
some important designs of RNNs:
\begin{itemize}
  \item RNNs which produce an output at every time step $\tau$ and are connected
    recurrently between hidden units.
  \item RNNs which produce an output at every time step $\tau$ and are connected
    recurrently only from the output from one time step between hidden units at
    another time step.
  \item RNNs which are connected recurrently between hidden units, read an
    entire data sequence $S$ and produce only one single output.
\end{itemize}

\input{sections/scientific/fr2/noutputrnn.tex}

We pick the RNN represented in figure~\ref{noutputrnn} to develop a forward
propagation equations. This RNN takes in a data sequence $S$ and outputs for
every time step $\tau$ an output. In this figure, we didn't include an
activation function for the hidden units $h$. However, for the equations, we
assume the activation function $\mathcal{H}$ to be the hyperbolic tangent.\\

Let $h^{(0)}$ be the initial state. For every time step from $t = 1$ to $t =
\tau$, we apply the following equations:
  \begin{align}
    \bm{a}^{(t)} & = \bm{b} + \bm{Wh}^{(t-1)} + \bm{Ux}^{(t)} \\
    \bm{h}^{(t)} & = \tanh(\bm{a}^{(t)}) \\
    \bm{o}^{(t)} & = \bm c + \bm{Vh}^{(V)} \\
    \bm{\hat y}^{(t)} & = softmax(\bm o^{(t)})
  \end{align}
where $\bm b$ and $\bm c$ are the bias vectors and $\bm U$, $\bm V$ and $\bm W$ are
the weight vectors.\\

%problem

\textbf{The Challenge of Long-Term Dependencies.} 

The challenge of learning long-term dependencies is that gradients, propagated
through a very deep RNN, can vanish or explode. Gradients can vanish when the
gradients are converging to $0$ and therefore the weights won't be updated while
learning. Exploding gradients happen if the gradients are too large and the loss
function will never reach the optimized minimum. There is a solution to this
problem. We will introduce Long Short-term Memory (LSTM) RNNs which are an RNN
architecture dealing with this challenge.\cite{doi:10.1162/neco.1997.9.8.1735}
