% corrected VD

\subsubsection{Recurrent Neural Network}~\\

% explain the difference between mlp and recurrent
% emphasize

The idea behind RNNs is to make use of sequential information. In a traditional
neural network we assume that all inputs (and outputs) are independent of each
other. But for many tasks that’s a very bad idea. If you want to predict the
next word in a sentence you better know which words came before it. RNNs are
called recurrent because they perform the same task for every element of a
sequence, with the output being depended on the previous computations and you
already know that they have a “memory” which captures information about what
has been calculated so far.

%introduction
Recurrent Neural Networks (RNNs) are a particular architecture of ANNs which can
process sequential data $S = x_{1},\dots,x_{\tau}$. To explain how RNNs are
implemented, we have to introduce the notion of computational graphs.
A computational graph is a formal structure representing a set of computations.
We obtain a chain of events by unfolding a recurrent computation into a
computational graph with a repetitive structure. \\

Consider a recurrent system,
\begin{equation}
  s_{t} = f(s_{t-1};\theta)
\end{equation}
with $s$ being the state of the system. The graph can be unfolded considering
the time step $\tau = 3$, we have,
\begin{equation}
  \begin{split}
    s_{3} & = f(s_{2};\theta) \\
          & = f(f(s_{1};\theta);\theta)
  \end{split}
  \label{recurrentsystem}
\end{equation}
This expression can be illustrated by a directed acyclic computational graph.
See figure~\ref{dag}\\
\input{sections/scientific/fr2/DAG.tex}

Now we consider recurrent system accepting an external signal $\bm{x}_{t}$,
\begin{equation}
  s^{t} = f(s^{t-1},\bm{x}_{t};\theta)
  \label{eqaccepting}
\end{equation}
where the state $s$ is containing information about the past sequence $S$. The
hidden units can be described with equation~\ref{eqaccepting} using $\bm{h}$ to
denote the state,
\begin{equation}
  \bm{h}_{t} = f(\bm{h}_{t-1},\bm{x}_{t};\theta)
\end{equation}
represented in figure~\ref{unfoldrnnaccepting}.\\
\input{sections/scientific/fr2/unfoldrnnaccepting.tex}

%math

With graph unfolding, we can now present some common examples of an RNN. Here are
some important designs of RNNs:

% rephrase

\begin{itemize}
  \item RNNs which produce an output at every time step $\tau$ and are connected
    recurrently between hidden units.
  \item RNNs which produce an output at every time step $\tau$ and are connected
    recurrently only from the output from one time step between hidden units at
    another time step.
  \item RNNs which are connected recurrently between hidden units, read an
    entire data sequence $S$ and produce only one single output.
\end{itemize}

\input{sections/scientific/fr2/noutputrnn.tex}

We pick the RNN represented in figure~\ref{noutputrnn} to develop forward
propagation equations. This RNN takes in a data sequence $S$ and outputs for
every time step $\tau$ an output. In this figure, we didn't include an
activation function for the hidden units $h$. However, for the equations, we
assume the activation function $\mathcal{H}$ to be the hyperbolic tangent.\\

Let $h_{0}$ be the initial state. For every time step from $t = 1$ to $t =
\tau$, we apply the following equations:
  \begin{align}
    \bm{a}_{t} & = \bm{b} + \bm{Wh}_{t-1} + \bm{Ux}_{t} \\
    \bm{h}_{t} & = \tanh(\bm{a}_{t}) \\
    \bm{o}_{t} & = \bm c + \bm{Vh}_{t} \\
    \bm{\hat y}_{t} & = softmax(\bm o_{t})
  \end{align}
where $\bm b$ and $\bm c$ are the bias vectors and $\bm U$, $\bm V$ and $\bm W$ are
the weight matrices.\\

%problem

\textbf{The Challenge of Long-Term Dependencies.} The challenge of learning
long-term dependencies is that gradients, propagated through a very deep RNN,
can vanish or explode. Gradients can vanish when the gradients are converging to
$0$ and therefore the weights won't be updated while learning. Exploding
gradients happen if the gradients are too large and the loss function will never
reach the optimized minimum. There is a solution to this problem. We will
introduce Long Short-term Memory (LSTM) and Gated Recurrent Unit (GRU) RNNs
which are an RNN architecture dealing with this
challenge.\cite{doi:10.1162/neco.1997.9.8.1735}
