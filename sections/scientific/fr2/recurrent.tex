% corrected VD 81

\subsubsection{Recurrent Neural Network}~\\

%introduction

Recurrent Neural Networks (RNNs) are a particular architecture of ANNs which can
process sequential data $S = x_{1},\dots,x_{n}$. In traditional ANN the input
and output data are assumed to be independent. However, for sequential data,
this won't be useful. For example, the prediction of the next word in a sentence
needs the information of which words appeared before. As opposed to MLP, RNN
feeds back their outputs back to their network and thus gain an overview of past
inputs. To explain how RNNs are implemented, we have to introduce the notion of
computational graphs. A computational graph is a formal structure representing a
set of computations. We obtain a chain of events by unfolding a recurrent
computation into a computational graph with a repetitive structure. \\

Consider a recurrent system,
\begin{equation}
  s_{t} = f(s_{t-1};\theta)
\end{equation}
with $s$ being the state of the system. The graph can be unfolded considering
the time step $t = 3$, we have,
\begin{equation}
  \begin{split}
    s_{3} & = f(s_{2};\theta) \\
          & = f(f(s_{1};\theta);\theta)
  \end{split}
  \label{recurrentsystem}
\end{equation}
This expression can be illustrated by a directed acyclic computational graph.
See figure~\ref{dag}\\
\input{sections/scientific/fr2/DAG.tex}

Now we consider the recurrent system accepting an external signal $\bm{x}_{t}$,
\begin{equation}
  s^{t} = f(s^{t-1},\bm{x}_{t};\theta)
  \label{eqaccepting}
\end{equation}
where the state $s$ is containing information about the past sequence $S$. The
hidden units can be described with equation~\ref{eqaccepting} using $\bm{h}$ to
denote the state,
\begin{equation}
  \bm{h}_{t} = f(\bm{h}_{t-1},\bm{x}_{t};\theta)
\end{equation}
represented in figure~\ref{unfoldrnnaccepting}.\\
\input{sections/scientific/fr2/unfoldrnnaccepting.tex}

%math

With graph unfolding, we can now present some common examples of an RNN. Here are
some important designs of RNNs:\\

\begin{itemize}
  \item RNNs which take a single input and produce an output at every time step
    $t$. This is called a one to many RNN.\\
  \item RNNs which read an entire data sequence and produce a data sequence.
    This is called a many to many RNN.\\
  \item RNNs which are connected recurrently between hidden units, read an
    entire data sequence $S$ and produce only one single output. This is called
    a many to one RNN.\\
\end{itemize}

\input{sections/scientific/fr2/noutputrnn.tex}

We pick the RNN represented in figure~\ref{noutputrnn} to develop forward
propagation equations. This RNN takes in a data sequence $S$ and outputs for
every time step $t$ an output. In this figure, we didn't include an
activation function for the hidden units $h$. However, for the equations, we
assume the activation function $\mathcal{H}$ to be the hyperbolic tangent.\\

Let $h_{0}$ be the initial state. For every time step from $t = 1$ to $t =
n$, we apply the following equations:
\begin{align}
  \bm{a}_{t} & = \bm{b} + \bm{Wh}_{t-1} + \bm{Ux}_{t} \\
  \bm{h}_{t} & = \tanh(\bm{a}_{t}) \\
  \bm{o}_{t} & = \bm c + \bm{Vh}_{t} \\
  \bm{\hat y}_{t} & = softmax(\bm o_{t})
\end{align}
where $\bm b$ and $\bm c$ are the bias vectors and $\bm U$, $\bm V$ and $\bm W$ are
the weight matrices.\\

%problem

\textbf{The Challenge of Long-Term Dependencies.} The challenge of learning
long-term dependencies is that gradients, propagated through a very deep RNN,
can vanish or explode. Gradients can vanish when the gradients are converging to
$0$ and therefore the weights won't be updated while learning. Exploding
gradients happen if the gradients are too large and the loss function will never
reach the optimized minimum. There is a solution to this problem. We will
introduce Long Short-term Memory (LSTM) and Gated Recurrent Unit (GRU) RNNs
which are an RNN architecture dealing with this
challenge.\cite{doi:10.1162/neco.1997.9.8.1735}~\\
