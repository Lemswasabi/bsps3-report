% corrected LN 86
\subsubsection{Implementation of classification models}\label{implementation}~\\

In this section, the implementation of the four ANN models are presented.\\

First of all, we initialize the training and testing sets:
\lstinputlisting{sections/technical/fr1/input.py}

\begin{enumerate}[label=\arabic*.]
  \item Feedforward neural networks:
    \lstinputlisting{sections/technical/fr1/feedforward.py}
    The sequential model is used to stack four \textit{Dense} layers, which are
    fully connected layers, meaning that all neurons in one layer are connected
    to all neurons in the subsequent layer. This model is composed of one input
    layer, two hidden layers and one output layer. The first dense layer takes
    as input a vector with the same dimensions as a row in the dataset. The
    first three layers are using \textit{ReLU} as activation function whereas
    the output layer is using \textit{softmax}, which is a standard choice for
    multi-class classification problems. In between the dense layers, dropout
    layers are stacked. These dropout layers are dropping units from the model
    to prevent overfitting. The parameter \textit{loss} of the
    \textit{.compile()} function is set to
    \textit{'sparse\_categorical\_crossentropy'} since the output should be an
    integer tensor instead of a binary vector.\\

  \begin{normalize}
    Before continuing with the recurrent models, the input data has to be
    reshaped since RNN, LSTM and GRU are taking in 3D shaped input data:

    \lstinputlisting{sections/technical/fr1/inputchange.py}
  \end{normalize}

  \item RNN:
    \lstinputlisting{sections/technical/fr1/rnn.py}
    The RNN model is implemented in the same manner as the feedforward model.
    Although, this model is only stacked with one hidden layer.  Instead of
    using dense layers, this model uses \textit{SimpleRNN} layers.  The
    parameter \textit{return\_sequences} is set to \textit{True} so that the
    next simpleRNN layer has access to all the hidden states from the previous
    layer.  The output layer is again a dense layer with \textit{softmax} as the
    activation function. As input, it takes in a matrix, which is an instance of
    the 3D dataset.~\\

\newpage

  \item LSTM:
    \lstinputlisting{sections/technical/fr1/lstm.py}
    The LSTM model uses \textit{LSTM} layers and is composed of one hidden layer
    as well. Its parameter \textit{return\_sequences} is set to \textit{True}
    with the same reason as for the RNN model. The LSTM model has the same
    output layer as the two previous ones.\\
  \item GRU:
    \lstinputlisting{sections/technical/fr1/gru.py}
    The GRU model uses \textit{GRU} layers and is composed of one hidden layer.
    Its parameters are set to the same values as the RNN and LSTM model. It uses
    the same dense layer activated by the \textit{softmax} function as output
    layer.
\end{enumerate}
