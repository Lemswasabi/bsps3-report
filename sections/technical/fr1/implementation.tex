% corrected LN 86
\subsubsection{Implementation of classification models}\label{implementation}~\\

In this section, the implementation of the four ANN models are presented.\\

First of all, we initialize the training and testing sets:
\lstinputlisting{sections/technical/fr1/input.py}

\begin{enumerate}[label=\arabic*.]
  \item Feedforward:
    \lstinputlisting{sections/technical/fr1/feedforward.py}
    The sequential model is used to stack four \textit{Dense} layers. The first
    dense layer takes as input a vector with the same dimensions as a row in the
    dataset. The first three layers are using \textit{ReLU} as activation
    function whereas the output layer is using \textit{softmax}. In between the
    dense layers, dropout layers are stacked. Theses dropout layers are dropping
    units from the model to prevent overfitting. The parameter \textit{loss} of
    the \textit{.compile()} function is set to
    \textit{'sparse\_categorical\_crossentropy'} since the output should be an
    integer tensor instead of a binary vector.\\

  \begin{normalize}
    Before continuing with the recurrent models, the input data has to be
    reshaped since RNN, LSTM and GRU are taking in 3D shaped input data:

    \lstinputlisting{sections/technical/fr1/inputchange.py}
  \end{normalize}

  \item RNN:
    \lstinputlisting{sections/technical/fr1/rnn.py}
    The RNN model is implemented in the same manner as the feedforward model.
    Instead of using dense layers, this model uses \textit{SimpleRnn} layers.
    The parameter \textit{return\_sequences} is set to \textit{True} so that the
    next simpleRNN layer has access to all the hidden states from the previous
    layer.  The output layer is again a dense layer with \textit{softmax} as the
    activation function. As input, it takes in a matrix, which is an instance of
    the 3D dataset.~\\

  \item LSTM:
    \lstinputlisting{sections/technical/fr1/lstm.py}
    The LSTM model uses \textit{LSTM} layers. Its parameter
    \textit{return\_sequences} is set to \textit{True} with the same reason as
    for the RNN model. The LSTM model has the same output layer as the two
    previous ones.\\
  \item GRU:
    \lstinputlisting{sections/technical/fr1/gru.py}
    The GRU model uses \textit{GRU} layers. Its parameters are set to the same
    values as the RNN and LSTM model. It uses the same dense layer activated by
    the \textit{softmax} function as output layer.
\end{enumerate}
